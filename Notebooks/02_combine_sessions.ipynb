{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 1 — imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, numpy as np, torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# raw data root (unzipped from 00)\n",
    "RAW_ROOT = \"../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final\"\n",
    "\n",
    "# where to save processed combined datasets\n",
    "PROC_ROOT = \"../data/processed/brain-to-text-25\"\n",
    "os.makedirs(PROC_ROOT, exist_ok=True)\n",
    "\n",
    "SPLITS = [\"data_train.hdf5\", \"data_val.hdf5\", \"data_test.hdf5\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 2 — discover all split files across days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train.hdf5 45 files\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_train.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_train.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_train.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_train.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_train.hdf5\n",
      "data_val.hdf5 41 files\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_val.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_val.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_val.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_val.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_val.hdf5\n",
      "data_test.hdf5 41 files\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_test.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_test.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_test.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_test.hdf5\n",
      "   ../data/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_test.hdf5\n"
     ]
    }
   ],
   "source": [
    "def discover_split_files(raw_root, split_name):\n",
    "    files = []\n",
    "    for day in sorted(os.listdir(raw_root)):\n",
    "        day_path = os.path.join(raw_root, day)\n",
    "        if not os.path.isdir(day_path):\n",
    "            continue\n",
    "        fp = os.path.join(day_path, split_name)\n",
    "        if os.path.exists(fp):\n",
    "            files.append(fp)\n",
    "    return files\n",
    "\n",
    "files_by_split = {s: discover_split_files(RAW_ROOT, s) for s in SPLITS}\n",
    "\n",
    "for s, flist in files_by_split.items():\n",
    "    print(s, len(flist), \"files\")\n",
    "    for f in flist[:5]:\n",
    "        print(\"  \", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 3 — small helpers (trim labels, per-trial z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_label_padding(label_ids: np.ndarray, pad_id: int = 0) -> np.ndarray:\n",
    "    if label_ids.ndim == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    # drop trailing pad_id\n",
    "    idx = np.where(label_ids != pad_id)[0]\n",
    "    if idx.size == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    last = idx[-1] + 1\n",
    "    return label_ids[:last].astype(np.int64)\n",
    "\n",
    "def zscore_per_trial(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    # x: (time, features) -> normalize each feature within this trial\n",
    "    mu = x.mean(axis=0, keepdims=True)\n",
    "    sd = x.std(axis=0, keepdims=True)\n",
    "    return (x - mu) / (sd + eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 4 — load one split across all days; build unified lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_across_days(file_list, require_labels=True, normalize=True, max_time=None):\n",
    "    X_list, y_list, lengths = [], [], []\n",
    "    bad = 0\n",
    "\n",
    "    for fp in tqdm(file_list, desc=\"Loading\"):\n",
    "        try:\n",
    "            with h5py.File(fp, \"r\") as f:\n",
    "                for tkey in sorted(f.keys()):\n",
    "                    g = f[tkey]\n",
    "                    if \"input_features\" not in g:\n",
    "                        continue\n",
    "                    x = g[\"input_features\"][()]  # (T, F)\n",
    "\n",
    "                    if max_time is not None and x.shape[0] > max_time:\n",
    "                        x = x[:max_time]\n",
    "\n",
    "                    if normalize:\n",
    "                        x = zscore_per_trial(x)\n",
    "\n",
    "                    if require_labels and \"seq_class_ids\" in g:\n",
    "                        y = trim_label_padding(g[\"seq_class_ids\"][()])\n",
    "                    elif require_labels:\n",
    "                        # skip trials without labels in a labeled split\n",
    "                        continue\n",
    "                    else:\n",
    "                        # test split -> labels unknown\n",
    "                        y = np.array([], dtype=np.int64)\n",
    "\n",
    "                    X_list.append(x.astype(np.float32))\n",
    "                    y_list.append(y)\n",
    "                    lengths.append(x.shape[0])\n",
    "        except Exception as e:\n",
    "            bad += 1\n",
    "            print(f\"⚠️  Skipping {fp}: {e}\")\n",
    "\n",
    "    print(f\"Collected {len(X_list)} trials | skipped files: {bad}\")\n",
    "    return X_list, y_list, np.array(lengths, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 5 — build combined datasets (train/val/test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 45/45 [00:32<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 8072 trials | skipped files: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 41/41 [00:38<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1426 trials | skipped files: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 41/41 [00:25<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1450 trials | skipped files: 0\n",
      "Train: 8072 | Val: 1426 | Test: 1450\n",
      "Feature dim (train[0]): 512\n",
      "Trial length range (train): (np.int32(138), np.int32(2475))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tweak max_time if you want to hard-cap sequence length (e.g., max_time=1000). None = keep full length.\n",
    "MAX_TIME = None\n",
    "\n",
    "train_X, train_y, train_L = load_split_across_days(files_by_split[\"data_train.hdf5\"], require_labels=True,  max_time=MAX_TIME)\n",
    "val_X,   val_y,   val_L   = load_split_across_days(files_by_split[\"data_val.hdf5\"],   require_labels=True,  max_time=MAX_TIME)\n",
    "test_X,  test_y,  test_L  = load_split_across_days(files_by_split[\"data_test.hdf5\"],  require_labels=False, max_time=MAX_TIME)\n",
    "\n",
    "print(\"Train:\", len(train_X), \"| Val:\", len(val_X), \"| Test:\", len(test_X))\n",
    "print(\"Feature dim (train[0]):\", train_X[0].shape[1] if train_X else None)\n",
    "print(\"Trial length range (train):\", (train_L.min() if train_L.size else None, train_L.max() if train_L.size else None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 6 — save processed sets (torch .pt for convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.13.2)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def save_pt(path, X_list, y_list, lengths):\n",
    "    payload = {\"X\": X_list, \"y\": y_list, \"lengths\": lengths}\n",
    "    torch.save(payload, path)\n",
    "    size_mb = os.path.getsize(path) / (1024*1024)\n",
    "    print(f\"Saved {path} ({size_mb:.1f} MB)\")\n",
    "\n",
    "save_pt(os.path.join(PROC_ROOT, \"train.pt\"), train_X, train_y, train_L)\n",
    "save_pt(os.path.join(PROC_ROOT, \"val.pt\"),   val_X,   val_y,   val_L)\n",
    "save_pt(os.path.join(PROC_ROOT, \"test.pt\"),  test_X,  test_y,  test_L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 7 — PyTorch Dataset + collate (padding on the fly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class BrainTextCombined(Dataset):\n",
    "    def __init__(self, pt_path, require_labels=True):\n",
    "        blob = torch.load(pt_path, map_location=\"cpu\")\n",
    "        self.X = blob[\"X\"]    # list of np arrays (T,F)\n",
    "        self.y = blob[\"y\"]    # list of np arrays (L,)\n",
    "        self.require_labels = require_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])             # (T,F) float32\n",
    "        if self.require_labels:\n",
    "            y = torch.from_numpy(self.y[idx]).long()  # (L,)\n",
    "        else:\n",
    "            y = torch.empty(0, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    x_lens = torch.tensor([x.size(0) for x in xs], dtype=torch.int32)\n",
    "    y_lens = torch.tensor([y.size(0) for y in ys], dtype=torch.int32)\n",
    "\n",
    "    xs_pad = pad_sequence(xs, batch_first=True)  # (B, T_max, F)\n",
    "    ys_pad = pad_sequence(ys, batch_first=True)  # (B, L_max)\n",
    "\n",
    "    return xs_pad, x_lens, ys_pad, y_lens\n",
    "\n",
    "# example loaders\n",
    "train_ds = BrainTextCombined(os.path.join(PROC_ROOT, \"train.pt\"), require_labels=True)\n",
    "val_ds   = BrainTextCombined(os.path.join(PROC_ROOT, \"val.pt\"),   require_labels=True)\n",
    "test_ds  = BrainTextCombined(os.path.join(PROC_ROOT, \"test.pt\"),  require_labels=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  collate_fn=collate_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 8 — quick sanity check (shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, xl, yb, yl = next(iter(train_loader))\n",
    "print(\"X batch:\", xb.shape, \"| x_lens:\", xl[:5].tolist())\n",
    "print(\"Y batch:\", yb.shape, \"| y_lens:\", yl[:5].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
